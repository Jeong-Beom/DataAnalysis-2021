{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-patent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415uhIj6DA7G"
      },
      "source": [
        "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMGCqb408mLS"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDa_rYJZ-WER",
        "outputId": "59bcf256-e597-4870-dad2-c28111a42c03"
      },
      "source": [
        "%%writefile utils.py\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from itertools import chain\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "RANDOM_STATE = 50\n",
        "TRAIN_FRACTION = 0.7\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    \"\"\"Retrieve a Keras model and embeddings\"\"\"\n",
        "    model = load_model(f'../models/{model_name}.h5')\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    word_idx = []\n",
        "    with open(f'../data/training-rnn.json', 'rb') as f:\n",
        "        for l in f:\n",
        "            word_idx.append(json.loads(l))\n",
        "        \n",
        "    word_idx = word_idx[0]\n",
        "    word_idx['UNK'] = 0\n",
        "    idx_word = {index: word for word, index in word_idx.items()}\n",
        "    return model, embeddings, word_idx, idx_word\n",
        "\n",
        "def get_embeddings(model):\n",
        "    \"\"\"Retrieve the embeddings in a model\"\"\"\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    return embeddings\n",
        "    \n",
        "def find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n",
        "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
        "    \n",
        "    idx = word_idx.get(query, None)\n",
        "    # Handle case where query is not in vocab\n",
        "    if idx is None:\n",
        "        print(f'{query} not found in vocab.')\n",
        "        return\n",
        "    else:\n",
        "        vec = embedding_matrix[idx]\n",
        "        # Handle case where word doesn't have an embedding\n",
        "        if np.all(vec == 0):\n",
        "            print(f'{query} has no pre-trained embedding.')\n",
        "            return\n",
        "        else:\n",
        "            # Calculate distance between vector and all others\n",
        "            dists = np.dot(embedding_matrix, vec)\n",
        "            \n",
        "            # Sort indexes in reverse order\n",
        "            idxs = np.argsort(dists)[::-1][:n]\n",
        "            sorted_dists = dists[idxs]\n",
        "            closest = [idx_word[i] for i in idxs]\n",
        "            \n",
        "    print(f'Query: {query}\\n')\n",
        "    # Print out the word and cosine distances\n",
        "    for word, dist in zip(closest, sorted_dists):\n",
        "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')\n",
        "        \n",
        "def format_sequence(s):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "    \n",
        "    # Add spaces around punctuation\n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    \n",
        "    # Remove references to figures\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    \n",
        "    # Remove double spaces\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    return s\n",
        "\n",
        "def remove_spaces(s):\n",
        "    \"\"\"Remove spaces around punctuation\"\"\"\n",
        "    s = re.sub(r'\\s+([.,;?])', r'\\1', s)\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
        "             lower=False):\n",
        "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
        "    \n",
        "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
        "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
        "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
        "        abstracts, training_len, lower, filters)\n",
        "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
        "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
        "                     'y_train': y_train, 'y_valid': y_valid}\n",
        "    return training_dict, word_idx, idx_word, sequences\n",
        "\n",
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       num_words,\n",
        "                       train_fraction=0.7):\n",
        "    \"\"\"Create training and validation features and labels.\"\"\"\n",
        "    \n",
        "    # Randomly shuffle features and labels\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Decide on number of samples for training\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "    # Using int8 for memory savings\n",
        "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # One hot encoding of labels\n",
        "    for example_index, word_index in enumerate(train_labels):\n",
        "        y_train[example_index, word_index] = 1\n",
        "\n",
        "    for example_index, word_index in enumerate(valid_labels):\n",
        "        y_valid[example_index, word_index] = 1\n",
        "\n",
        "    # Memory management\n",
        "    import gc\n",
        "    gc.enable()\n",
        "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid\n",
        "\n",
        "def make_sequences(texts, training_length = 50,\n",
        "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "    \n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "    \n",
        "    print(f'There are {num_words} unique words.')\n",
        "    \n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    \n",
        "    # Limit to sequences with more than training length tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
        "    \n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "    \n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "        \n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "        \n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length: i + 1]\n",
        "            \n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "    \n",
        "    print(f'There are {len(features)} sequences.')\n",
        "    \n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
        "\n",
        "def generate_output(model,\n",
        "                    sequences,\n",
        "                    idx_word,\n",
        "                    seed_length=50,\n",
        "                    new_words=50,\n",
        "                    diversity=1,\n",
        "                    return_output=False,\n",
        "                    n_gen=1):\n",
        "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
        "\n",
        "    # Choose a random sequence\n",
        "    seq = random.choice(sequences)\n",
        "\n",
        "    # Choose a random starting point\n",
        "    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "    # Ending index for seed\n",
        "    end_idx = seed_idx + seed_length\n",
        "\n",
        "    gen_list = []\n",
        "\n",
        "    for n in range(n_gen):\n",
        "        # Extract the seed sequence\n",
        "        seed = seq[seed_idx:end_idx]\n",
        "        original_sequence = [idx_word[i] for i in seed]\n",
        "        generated = seed[:] + ['#']\n",
        "\n",
        "        # Find the actual entire sequence\n",
        "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "\n",
        "        # Keep adding new words\n",
        "        for i in range(new_words):\n",
        "\n",
        "            # Make a prediction from the seed\n",
        "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
        "                np.float64)\n",
        "\n",
        "            # Diversify\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "\n",
        "            # Softmax\n",
        "            preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "            # Choose the next word\n",
        "            probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "            next_idx = np.argmax(probas)\n",
        "\n",
        "            # New seed adds on old word\n",
        "            #             seed = seed[1:] + [next_idx]\n",
        "            seed += [next_idx]\n",
        "            generated.append(next_idx)\n",
        "\n",
        "        # Showing generated and actual abstract\n",
        "        n = []\n",
        "\n",
        "        for i in generated:\n",
        "            n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "        gen_list.append(n)\n",
        "\n",
        "    a = []\n",
        "\n",
        "    for i in actual:\n",
        "        a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    a = a[seed_length:]\n",
        "\n",
        "    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "    if return_output:\n",
        "        return original_sequence, gen_list, a\n",
        "\n",
        "    # HTML formatting\n",
        "    seed_html = ''\n",
        "    seed_html = addContent(seed_html, header(\n",
        "        'Seed Sequence', color='darkblue'))\n",
        "    seed_html = addContent(seed_html,\n",
        "                           box(remove_spaces(' '.join(original_sequence))))\n",
        "\n",
        "    gen_html = ''\n",
        "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
        "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
        "\n",
        "    a_html = ''\n",
        "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
        "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
        "\n",
        "    return seed_html, gen_html, a_html\n",
        "\n",
        "\n",
        "\n",
        "def header(text, color = 'black', gen_text = None):\n",
        "    if gen_text:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><p><center>' + str(\n",
        "        text) + '<span style=\"color: red\">' + str(gen_text) + '</center></p></h1>'\n",
        "    else:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><center>' + str(\n",
        "            text) + '</center></h1>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def box(text, gen_text=None):\n",
        "    if gen_text:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>' + str(\n",
        "            text) +'<span style=\"color: red\">' + str(gen_text) + '</p></div>'\n",
        "\n",
        "    else:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + str(\n",
        "            text) + '</div>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def addContent(old_html, raw_html):\n",
        "    old_html += raw_html\n",
        "    return old_html\n",
        "\n",
        "def seed_sequence(model, s, word_idx, idx_word, \n",
        "                  diversity = 0.75, num_words = 50):\n",
        "    \"\"\"Generate output starting from a seed sequence.\"\"\"\n",
        "    # Original formated text\n",
        "    start = format_sequence(s).split()\n",
        "    gen = []\n",
        "    s = start[:]\n",
        "    # Generate output\n",
        "    for _ in range(num_words):\n",
        "        # Conver to arry\n",
        "        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n",
        "\n",
        "        # Make predictions\n",
        "        preds = model.predict(x)[0].astype(float)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        # Softmax\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        # Pick next index\n",
        "        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n",
        "        s.append(idx_word[next_idx])\n",
        "        gen.append(idx_word[next_idx])\n",
        "    \n",
        "    # Formatting in html\n",
        "    start = remove_spaces(' '.join(start)) + ' '\n",
        "    gen = remove_spaces(' '.join(gen)) \n",
        "    html = ''\n",
        "    html = addContent(html, header('Input Seed ', color = 'black', gen_text = 'Network Output'))\n",
        "    html = addContent(html, box(start, gen))\n",
        "    return html\n",
        "\n",
        "def guess_human(model, sequences, idx_word, seed_length=50):\n",
        "    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n",
        "       Diversity is randomly set between 0.5 and 1.25\"\"\"\n",
        "    \n",
        "    new_words = np.random.randint(10, 50)\n",
        "    diversity = np.random.uniform(0.5, 1.25)\n",
        "    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n",
        "                                                 diversity=diversity, return_output=True, n_gen = 2)\n",
        "    gen_0, gen_1 = gen_list\n",
        "    \n",
        "    output = {'sequence': remove_spaces(' '.join(sequence)),\n",
        "              'computer0': remove_spaces(' '.join(gen_0)),\n",
        "              'computer1': remove_spaces(' '.join(gen_1)),\n",
        "              'human': remove_spaces(' '.join(actual))}\n",
        "    \n",
        "    print(f\"Seed Sequence: {output['sequence']}\\n\")\n",
        "    \n",
        "    choices = ['human', 'computer0', 'computer1']\n",
        "          \n",
        "    selected = []\n",
        "    i = 0\n",
        "    while len(selected) < 3:\n",
        "        choice = random.choice(choices)\n",
        "        selected.append(choice)\n",
        "        print(f'\\nOption {i + 1} {output[choice]}')\n",
        "        choices.remove(selected[-1])\n",
        "        i += 1\n",
        "    \n",
        "    print('\\n')\n",
        "    guess = int(input('Enter option you think is human (1-3): ')) - 1\n",
        "    print('\\n')\n",
        "    \n",
        "    if guess == np.where(np.array(selected) == 'human')[0][0]:\n",
        "        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Ordering: ', selected)\n",
        "    else:\n",
        "        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Correct Ordering: ', selected)\n",
        "          \n",
        "    print('Diversity', round(diversity, 2))\n",
        "    \n",
        "def make_sequences_new(texts,\n",
        "                   training_length=50,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Limit to sequences with more than (training length + 20) tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
        "    ]\n",
        "\n",
        "    new_texts = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "    \n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    # Refit on long texts\n",
        "    tokenizer.fit_on_texts(new_texts)\n",
        "    new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length:i + 1]\n",
        "\n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "\n",
        "    print(f'There are {len(features)} training sequences.')\n",
        "\n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kldVzc8-5q"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import HTML\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category = UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YD7o-Da_8-8a",
        "outputId": "9fca71c1-960d-4ff0-ff1f-910bb16f1eed"
      },
      "source": [
        "data = pd.read_csv('drive/MyDrive/RNN-example/neural_network_patent_query.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patent_abstract</th>\n",
              "      <th>patent_date</th>\n",
              "      <th>patent_number</th>\n",
              "      <th>patent_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
              "      <td>1996-07-09</td>\n",
              "      <td>5535303</td>\n",
              "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" This invention is a novel high-speed neural ...</td>\n",
              "      <td>1993-10-19</td>\n",
              "      <td>5255349</td>\n",
              "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An optical information processor for use as a ...</td>\n",
              "      <td>1995-01-17</td>\n",
              "      <td>5383042</td>\n",
              "      <td>3 layer liquid crystal neural network with out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2001-01-02</td>\n",
              "      <td>6169981</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>6581048</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     patent_abstract  ...                                       patent_title\n",
              "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  ...        \"\"\"Barometer\"\" neuron for a neural network\"\n",
              "1  \" This invention is a novel high-speed neural ...  ...  \"Electronic neural network for solving \"\"trave...\n",
              "2  An optical information processor for use as a ...  ...  3 layer liquid crystal neural network with out...\n",
              "3  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "4  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMahheId8-_M",
        "outputId": "7b11f847-92d4-417d-8a8c-3b97006e1920"
      },
      "source": [
        "training_dict, word_idx, idx_word, sequences = get_data('drive/MyDrive/RNN-example/neural_network_patent_query.csv', training_len = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 16192 unique words.\n",
            "There are 318563 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "503kKp2n8_B1",
        "outputId": "49ec909f-6767-4209-c7a3-39058ef8cd48"
      },
      "source": [
        "training_dict['X_train'][:2]\n",
        "training_dict['y_train'][:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
              "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
              "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
              "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
              "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
              "           81,   183,     4,    13,   117],\n",
              "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
              "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
              "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
              "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
              "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
              "         9292,   244,    14,    22, 13944]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdIMO7ye8_Ep",
        "outputId": "ca9abefa-bade-4c6e-8229-c54820155ecc"
      },
      "source": [
        "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
        "    text = []\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text) + '\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
            "\n",
            "Label: is\n",
            "\n",
            "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
            "\n",
            "Label: group\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOFw88g8_HT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OSLgaqr8_J-",
        "outputId": "23287639-3185-4e52-f93c-51e89b56ab8b"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=len(word_idx) + 1, output_dim=100, weights=None, trainable=True)) # Embedding layer\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))   # Recurrent layer\n",
        "model.add(Dense(64, activation='relu'))   # Fully connected layer\n",
        "model.add(Dropout(0.5))   # Dropout for regularization\n",
        "model.add(Dense(len(word_idx) + 1, activation='softmax'))   # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         1619200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16192)             1052480   \n",
            "=================================================================\n",
            "Total params: 2,718,080\n",
            "Trainable params: 2,718,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFw6XQRg8_Mw",
        "outputId": "7d2d2fb8-001b-4dd8-b254-4f678875280e"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load in model and demonstrate training\n",
        "model = load_model('drive/MyDrive/RNN-example/models/train-embeddings-rnn.h5')\n",
        "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
        "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
        "          verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "109/109 [==============================] - 333s 3s/step - loss: 3.7542 - accuracy: 0.2951 - val_loss: 5.1257 - val_accuracy: 0.2673\n",
            "Epoch 2/5\n",
            "109/109 [==============================] - 330s 3s/step - loss: 3.7396 - accuracy: 0.2965 - val_loss: 5.1417 - val_accuracy: 0.2681\n",
            "Epoch 3/5\n",
            "109/109 [==============================] - 330s 3s/step - loss: 3.7219 - accuracy: 0.2974 - val_loss: 5.1624 - val_accuracy: 0.2694\n",
            "Epoch 4/5\n",
            "109/109 [==============================] - 330s 3s/step - loss: 3.7133 - accuracy: 0.2982 - val_loss: 5.1641 - val_accuracy: 0.2698\n",
            "Epoch 5/5\n",
            "109/109 [==============================] - 330s 3s/step - loss: 3.6991 - accuracy: 0.3004 - val_loss: 5.1746 - val_accuracy: 0.2701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWy0FPG78_Pb",
        "outputId": "f1232434-a4a7-47a4-900a-fb91da854f63"
      },
      "source": [
        "model = load_model('drive/MyDrive/RNN-example/models/train-embeddings-rnn.h5')\n",
        "print('Model Performance: Log Loss and Accuracy on training data')\n",
        "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
        "\n",
        "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
        "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre-trained-rnn.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m53xAkDA8_VB"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
        "    HTML(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2UFDKnX8_X2"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
        "    HTML(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgIFoQPN8_ag"
      },
      "source": [
        "s = 'This patent provides a basis for using a recurrent neural network to '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJVbecKH8_dW"
      },
      "source": [
        "s = 'The cell state is passed along from one time step to another allowing the '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3pfGQjz8_f_"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQswppjZ8_ip"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAmxok08_lt"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQSbozpc8_oa"
      },
      "source": [
        "embeddings = get_embeddings(model)\n",
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNsaAbYj8_rH"
      },
      "source": [
        "find_closest('network', embeddings, word_idx, idx_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93hJKZ_YCDpm"
      },
      "source": [
        "find_closest('data', embeddings, word_idx, idx_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdN9XGrCDuM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSEGg-KCD0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4781cb75-f7a8-4ddb-eb4b-6e346b5f1bd9"
      },
      "source": [
        "import requests \n",
        "\n",
        "response = requests.get('https://google.com/') \n",
        "print(response) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpblypvDCD4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjiL7ZB6CD9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a013ade5-4d9d-4857-a841-2de629481311"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYmOQvV3CEB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d0a721-413b-4f9f-c2c9-cb5cd8e74309"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 72s 456ms/step - loss: 0.7296 - acc: 0.5204 - val_loss: 0.6856 - val_acc: 0.5744\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 71s 455ms/step - loss: 0.6686 - acc: 0.6603 - val_loss: 0.6711 - val_acc: 0.6370\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 71s 455ms/step - loss: 0.6370 - acc: 0.7475 - val_loss: 0.6386 - val_acc: 0.7004\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 72s 456ms/step - loss: 0.5750 - acc: 0.7964 - val_loss: 0.5536 - val_acc: 0.7778\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 71s 455ms/step - loss: 0.4633 - acc: 0.8263 - val_loss: 0.4567 - val_acc: 0.8184\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 71s 455ms/step - loss: 0.3744 - acc: 0.8595 - val_loss: 0.4215 - val_acc: 0.8436\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 72s 456ms/step - loss: 0.3251 - acc: 0.8827 - val_loss: 0.4043 - val_acc: 0.8580\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 72s 456ms/step - loss: 0.2909 - acc: 0.8957 - val_loss: 0.4271 - val_acc: 0.8602\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 71s 455ms/step - loss: 0.2605 - acc: 0.9090 - val_loss: 0.4643 - val_acc: 0.8574\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 71s 454ms/step - loss: 0.2343 - acc: 0.9186 - val_loss: 0.4605 - val_acc: 0.8698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWQE7QskCEGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7MId5LiCEMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_sjo5kCEP2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IAWDepA8_t3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}