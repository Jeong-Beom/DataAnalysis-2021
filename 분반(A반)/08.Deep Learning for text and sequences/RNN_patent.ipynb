{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415uhIj6DA7G"
      },
      "source": [
        "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JMGCqb408mLS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDa_rYJZ-WER",
        "outputId": "59bcf256-e597-4870-dad2-c28111a42c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from itertools import chain\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "RANDOM_STATE = 50\n",
        "TRAIN_FRACTION = 0.7\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    \"\"\"Retrieve a Keras model and embeddings\"\"\"\n",
        "    model = load_model('C:/Workspace/python/Data_Science/dataA/분반(A반)/Deep Learning for text and sequences/train-embeddings-rnn.h5')\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    word_idx = []\n",
        "    with open(f'../data/training-rnn.json', 'rb') as f:\n",
        "        for l in f:\n",
        "            word_idx.append(json.loads(l))\n",
        "        \n",
        "    word_idx = word_idx[0]\n",
        "    word_idx['UNK'] = 0\n",
        "    idx_word = {index: word for word, index in word_idx.items()}\n",
        "    return model, embeddings, word_idx, idx_word\n",
        "\n",
        "def get_embeddings(model):\n",
        "    \"\"\"Retrieve the embeddings in a model\"\"\"\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    return embeddings\n",
        "    \n",
        "def find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n",
        "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
        "    \n",
        "    idx = word_idx.get(query, None)\n",
        "    # Handle case where query is not in vocab\n",
        "    if idx is None:\n",
        "        print(f'{query} not found in vocab.')\n",
        "        return\n",
        "    else:\n",
        "        vec = embedding_matrix[idx]\n",
        "        # Handle case where word doesn't have an embedding\n",
        "        if np.all(vec == 0):\n",
        "            print(f'{query} has no pre-trained embedding.')\n",
        "            return\n",
        "        else:\n",
        "            # Calculate distance between vector and all others\n",
        "            dists = np.dot(embedding_matrix, vec)\n",
        "            \n",
        "            # Sort indexes in reverse order\n",
        "            idxs = np.argsort(dists)[::-1][:n]\n",
        "            sorted_dists = dists[idxs]\n",
        "            closest = [idx_word[i] for i in idxs]\n",
        "            \n",
        "    print(f'Query: {query}\\n')\n",
        "    # Print out the word and cosine distances\n",
        "    for word, dist in zip(closest, sorted_dists):\n",
        "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')\n",
        "        \n",
        "def format_sequence(s):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "    \n",
        "    # Add spaces around punctuation\n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    \n",
        "    # Remove references to figures\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    \n",
        "    # Remove double spaces\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    return s\n",
        "\n",
        "def remove_spaces(s):\n",
        "    \"\"\"Remove spaces around punctuation\"\"\"\n",
        "    s = re.sub(r'\\s+([.,;?])', r'\\1', s)\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
        "             lower=False):\n",
        "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
        "    \n",
        "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
        "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
        "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
        "        abstracts, training_len, lower, filters)\n",
        "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
        "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
        "                     'y_train': y_train, 'y_valid': y_valid}\n",
        "    return training_dict, word_idx, idx_word, sequences\n",
        "\n",
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       num_words,\n",
        "                       train_fraction=0.7):\n",
        "    \"\"\"Create training and validation features and labels.\"\"\"\n",
        "    \n",
        "    # Randomly shuffle features and labels\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Decide on number of samples for training\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "    # Using int8 for memory savings\n",
        "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # One hot encoding of labels\n",
        "    for example_index, word_index in enumerate(train_labels):\n",
        "        y_train[example_index, word_index] = 1\n",
        "\n",
        "    for example_index, word_index in enumerate(valid_labels):\n",
        "        y_valid[example_index, word_index] = 1\n",
        "\n",
        "    # Memory management\n",
        "    import gc\n",
        "    gc.enable()\n",
        "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid\n",
        "\n",
        "def make_sequences(texts, training_length = 50,\n",
        "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "    \n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "    \n",
        "    print(f'There are {num_words} unique words.')\n",
        "    \n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    \n",
        "    # Limit to sequences with more than training length tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
        "    \n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "    \n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "        \n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "        \n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length: i + 1]\n",
        "            \n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "    \n",
        "    print(f'There are {len(features)} sequences.')\n",
        "    \n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
        "\n",
        "def generate_output(model,\n",
        "                    sequences,\n",
        "                    idx_word,\n",
        "                    seed_length=50,\n",
        "                    new_words=50,\n",
        "                    diversity=1,\n",
        "                    return_output=False,\n",
        "                    n_gen=1):\n",
        "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
        "\n",
        "    # Choose a random sequence\n",
        "    seq = random.choice(sequences)\n",
        "\n",
        "    # Choose a random starting point\n",
        "    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "    # Ending index for seed\n",
        "    end_idx = seed_idx + seed_length\n",
        "\n",
        "    gen_list = []\n",
        "\n",
        "    for n in range(n_gen):\n",
        "        # Extract the seed sequence\n",
        "        seed = seq[seed_idx:end_idx]\n",
        "        original_sequence = [idx_word[i] for i in seed]\n",
        "        generated = seed[:] + ['#']\n",
        "\n",
        "        # Find the actual entire sequence\n",
        "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "\n",
        "        # Keep adding new words\n",
        "        for i in range(new_words):\n",
        "\n",
        "            # Make a prediction from the seed\n",
        "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
        "                np.float64)\n",
        "\n",
        "            # Diversify\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "\n",
        "            # Softmax\n",
        "            preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "            # Choose the next word\n",
        "            probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "            next_idx = np.argmax(probas)\n",
        "\n",
        "            # New seed adds on old word\n",
        "            #             seed = seed[1:] + [next_idx]\n",
        "            seed += [next_idx]\n",
        "            generated.append(next_idx)\n",
        "\n",
        "        # Showing generated and actual abstract\n",
        "        n = []\n",
        "\n",
        "        for i in generated:\n",
        "            n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "        gen_list.append(n)\n",
        "\n",
        "    a = []\n",
        "\n",
        "    for i in actual:\n",
        "        a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    a = a[seed_length:]\n",
        "\n",
        "    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "    if return_output:\n",
        "        return original_sequence, gen_list, a\n",
        "\n",
        "    # HTML formatting\n",
        "    seed_html = ''\n",
        "    seed_html = addContent(seed_html, header(\n",
        "        'Seed Sequence', color='darkblue'))\n",
        "    seed_html = addContent(seed_html,\n",
        "                           box(remove_spaces(' '.join(original_sequence))))\n",
        "\n",
        "    gen_html = ''\n",
        "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
        "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
        "\n",
        "    a_html = ''\n",
        "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
        "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
        "\n",
        "    return seed_html, gen_html, a_html\n",
        "\n",
        "\n",
        "\n",
        "def header(text, color = 'black', gen_text = None):\n",
        "    if gen_text:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><p><center>' + str(\n",
        "        text) + '<span style=\"color: red\">' + str(gen_text) + '</center></p></h1>'\n",
        "    else:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><center>' + str(\n",
        "            text) + '</center></h1>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def box(text, gen_text=None):\n",
        "    if gen_text:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>' + str(\n",
        "            text) +'<span style=\"color: red\">' + str(gen_text) + '</p></div>'\n",
        "\n",
        "    else:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + str(\n",
        "            text) + '</div>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def addContent(old_html, raw_html):\n",
        "    old_html += raw_html\n",
        "    return old_html\n",
        "\n",
        "def seed_sequence(model, s, word_idx, idx_word, \n",
        "                  diversity = 0.75, num_words = 50):\n",
        "    \"\"\"Generate output starting from a seed sequence.\"\"\"\n",
        "    # Original formated text\n",
        "    start = format_sequence(s).split()\n",
        "    gen = []\n",
        "    s = start[:]\n",
        "    # Generate output\n",
        "    for _ in range(num_words):\n",
        "        # Conver to arry\n",
        "        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n",
        "\n",
        "        # Make predictions\n",
        "        preds = model.predict(x)[0].astype(float)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        # Softmax\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        # Pick next index\n",
        "        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n",
        "        s.append(idx_word[next_idx])\n",
        "        gen.append(idx_word[next_idx])\n",
        "    \n",
        "    # Formatting in html\n",
        "    start = remove_spaces(' '.join(start)) + ' '\n",
        "    gen = remove_spaces(' '.join(gen)) \n",
        "    html = ''\n",
        "    html = addContent(html, header('Input Seed ', color = 'black', gen_text = 'Network Output'))\n",
        "    html = addContent(html, box(start, gen))\n",
        "    return html\n",
        "\n",
        "def guess_human(model, sequences, idx_word, seed_length=50):\n",
        "    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n",
        "       Diversity is randomly set between 0.5 and 1.25\"\"\"\n",
        "    \n",
        "    new_words = np.random.randint(10, 50)\n",
        "    diversity = np.random.uniform(0.5, 1.25)\n",
        "    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n",
        "                                                 diversity=diversity, return_output=True, n_gen = 2)\n",
        "    gen_0, gen_1 = gen_list\n",
        "    \n",
        "    output = {'sequence': remove_spaces(' '.join(sequence)),\n",
        "              'computer0': remove_spaces(' '.join(gen_0)),\n",
        "              'computer1': remove_spaces(' '.join(gen_1)),\n",
        "              'human': remove_spaces(' '.join(actual))}\n",
        "    \n",
        "    print(f\"Seed Sequence: {output['sequence']}\\n\")\n",
        "    \n",
        "    choices = ['human', 'computer0', 'computer1']\n",
        "          \n",
        "    selected = []\n",
        "    i = 0\n",
        "    while len(selected) < 3:\n",
        "        choice = random.choice(choices)\n",
        "        selected.append(choice)\n",
        "        print(f'\\nOption {i + 1} {output[choice]}')\n",
        "        choices.remove(selected[-1])\n",
        "        i += 1\n",
        "    \n",
        "    print('\\n')\n",
        "    guess = int(input('Enter option you think is human (1-3): ')) - 1\n",
        "    print('\\n')\n",
        "    \n",
        "    if guess == np.where(np.array(selected) == 'human')[0][0]:\n",
        "        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Ordering: ', selected)\n",
        "    else:\n",
        "        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Correct Ordering: ', selected)\n",
        "          \n",
        "    print('Diversity', round(diversity, 2))\n",
        "    \n",
        "def make_sequences_new(texts,\n",
        "                   training_length=50,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Limit to sequences with more than (training length + 20) tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
        "    ]\n",
        "\n",
        "    new_texts = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "    \n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    # Refit on long texts\n",
        "    tokenizer.fit_on_texts(new_texts)\n",
        "    new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length:i + 1]\n",
        "\n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "\n",
        "    print(f'There are {len(features)} training sequences.')\n",
        "\n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h5kldVzc8-5q"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import HTML\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category = UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YD7o-Da_8-8a",
        "outputId": "9fca71c1-960d-4ff0-ff1f-910bb16f1eed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patent_abstract</th>\n",
              "      <th>patent_date</th>\n",
              "      <th>patent_number</th>\n",
              "      <th>patent_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
              "      <td>1996-07-09</td>\n",
              "      <td>5535303</td>\n",
              "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" This invention is a novel high-speed neural ...</td>\n",
              "      <td>1993-10-19</td>\n",
              "      <td>5255349</td>\n",
              "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An optical information processor for use as a ...</td>\n",
              "      <td>1995-01-17</td>\n",
              "      <td>5383042</td>\n",
              "      <td>3 layer liquid crystal neural network with out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2001-01-02</td>\n",
              "      <td>6169981</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>6581048</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     patent_abstract patent_date  \\\n",
              "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
              "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
              "2  An optical information processor for use as a ...  1995-01-17   \n",
              "3  A method and system for intelligent control of...  2001-01-02   \n",
              "4  A method and system for intelligent control of...  2003-06-17   \n",
              "\n",
              "  patent_number                                       patent_title  \n",
              "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
              "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
              "2       5383042  3 layer liquid crystal neural network with out...  \n",
              "3       6169981  3-brain architecture for an intelligent decisi...  \n",
              "4       6581048  3-brain architecture for an intelligent decisi...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('C:/Workspace/python/Data_Science/dataA/분반(A반)/Deep Learning for text and sequences/neural_network_patent_query.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMahheId8-_M",
        "outputId": "7b11f847-92d4-417d-8a8c-3b97006e1920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 16192 unique words.\n",
            "There are 318563 sequences.\n"
          ]
        }
      ],
      "source": [
        "training_dict, word_idx, idx_word, sequences = get_data('C:/Workspace/python/Data_Science/dataA/분반(A반)/Deep Learning for text and sequences/neural_network_patent_query.csv', training_len = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "503kKp2n8_B1",
        "outputId": "49ec909f-6767-4209-c7a3-39058ef8cd48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
              "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
              "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
              "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
              "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
              "           81,   183,     4,    13,   117],\n",
              "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
              "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
              "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
              "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
              "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
              "         9292,   244,    14,    22, 13944]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_dict['X_train'][:2]\n",
        "training_dict['y_train'][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdIMO7ye8_Ep",
        "outputId": "ca9abefa-bade-4c6e-8229-c54820155ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
            "\n",
            "Label: is\n",
            "\n",
            "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
            "\n",
            "Label: group\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
        "    text = []\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text) + '\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nuOFw88g8_HT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OSLgaqr8_J-",
        "outputId": "23287639-3185-4e52-f93c-51e89b56ab8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         1619200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16192)             1052480   \n",
            "=================================================================\n",
            "Total params: 2,718,080\n",
            "Trainable params: 2,718,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=len(word_idx) + 1, output_dim=100, weights=None, trainable=True)) # Embedding layer\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))   # Recurrent layer\n",
        "model.add(Dense(64, activation='relu'))   # Fully connected layer\n",
        "model.add(Dropout(0.5))   # Dropout for regularization\n",
        "model.add(Dense(len(word_idx) + 1, activation='softmax'))   # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFw6XQRg8_Mw",
        "outputId": "7d2d2fb8-001b-4dd8-b254-4f678875280e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "109/109 [==============================] - 288s 3s/step - loss: 3.7554 - accuracy: 0.2950 - val_loss: 5.1398 - val_accuracy: 0.2677\n",
            "Epoch 2/5\n",
            "109/109 [==============================] - 322s 3s/step - loss: 3.7395 - accuracy: 0.2963 - val_loss: 5.1654 - val_accuracy: 0.2680\n",
            "Epoch 3/5\n",
            "109/109 [==============================] - 343s 3s/step - loss: 3.7233 - accuracy: 0.2972 - val_loss: 5.1611 - val_accuracy: 0.2685\n",
            "Epoch 4/5\n",
            "109/109 [==============================] - 347s 3s/step - loss: 3.7129 - accuracy: 0.2978 - val_loss: 5.1662 - val_accuracy: 0.2695\n",
            "Epoch 5/5\n",
            "109/109 [==============================] - 346s 3s/step - loss: 3.6977 - accuracy: 0.3001 - val_loss: 5.1821 - val_accuracy: 0.2698\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load in model and demonstrate training\n",
        "model = load_model('C:/Workspace/python/Data_Science/dataA/분반(A반)/Deep Learning for text and sequences/train-embeddings-rnn.h5')\n",
        "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
        "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
        "          verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWy0FPG78_Pb",
        "outputId": "f1232434-a4a7-47a4-900a-fb91da854f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Performance: Log Loss and Accuracy on training data\n",
            "109/109 [==============================] - 71s 646ms/step - loss: 3.2897 - accuracy: 0.3384\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3.289726734161377, 0.3384440839290619]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Performance: Log Loss and Accuracy on validation data\n",
            "47/47 [==============================] - 30s 631ms/step - loss: 5.1321 - accuracy: 0.2672\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[5.132135391235352, 0.2671891450881958]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = load_model('C:/Workspace/python/Data_Science/dataA/분반(A반)/Deep Learning for text and sequences/train-embeddings-rnn.h5')\n",
        "print('Model Performance: Log Loss and Accuracy on training data')\n",
        "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
        "\n",
        "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
        "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m53xAkDA8_VB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">.sup.-1 (5.56.mu.m) in which urea absorbs, at least one other determination being made in a waveband in which fat absorbs, at least one further determination being made in a waveband where lactose absorbs, and at least one further determination being made in a waveband</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- >, and such that the clothes or can be range. The decision is performed at the air conditioner and a vehicle. The data to determine the mobile station</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > where protein absorbs determining, on the basis of the thus determined attenuations and predetermined parameters established by multivariate calibration, the contribution from fat, lactose, and protein</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
        "    HTML(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-2UFDKnX8_X2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">color code training a neural network using the characteristic quantities and the texture evaluation values of the sample paint colors as training data and inputting characteristic quantities of the paint</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > template class parameters by teaching data of cells received until the output layer receives at least one other concept of the positions which may include a silicon phase part,</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > colors other than the sample paint colors into the neural network after the training, and storing output data after associating each output data with the paint color code.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
        "    HTML(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tgIFoQPN8_ag"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>This patent provides a basis for using a recurrent neural network to <span style=\"color: red\">A determination that includes the neural network. The combination of transfer value was addressed or undamaged input signals,</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = 'This patent provides a basis for using a recurrent neural network to '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qJVbecKH8_dW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>The cell state is passed along from one time step to another allowing the <span style=\"color: red\">second area for a plurality of output vector with the number of for a weighted summation unit (RS) and the</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = 'The cell state is passed along from one time step to another allowing the '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V3pfGQjz8_f_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed Sequence: a synapse cell coupled to a circuit for parallel implementation of weight adjustment provides the learning portion of the synaptic operation and includes a floating gate device having a corresponding floating gate member that stores the connection weight of the cell. Parallel weight adjustments are performed in a single\n",
            "\n",
            "\n",
            "Option 1 < --- > operational cycle utilizing floating gate technology and control signals that facilitate programming/erasing operations.\n",
            "\n",
            "Option 2 < --- > connection cell interval. The row discrete transfer function is generates the aggregate response\n",
            "\n",
            "Option 3 < --- > neural network. The binaural map. The capacitor signal can be used to\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***Correct***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ordering:  ['human', 'computer0', 'computer1']\n",
            "Diversity 0.9\n"
          ]
        }
      ],
      "source": [
        "guess_human(model, sequences, idx_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FQswppjZ8_ip"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed Sequence: self managed ad hoc communications network nodes and node mobility management. Nodes include an Artificial Neural Network (ANN) that determines connection to other network nodes. The ANN may use free space propagation link life estimation, inverse modeling for partition prediction, Stochastic Approximation, and/or coarse estimation\n",
            "\n",
            "\n",
            "Option 1 < --- > input nodes are preprocessed, where nodes are then based on the spoken image irrespective is determined\n",
            "\n",
            "Option 2 < --- >. The node includes storage storing network tables and matrices indicating network connectivity and connection to other\n",
            "\n",
            "Option 3 < --- > channels being input data data applied with arithmetic neurons the combined connections in the reduced memory.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***Correct***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ordering:  ['computer1', 'human', 'computer0']\n",
            "Diversity 1.13\n"
          ]
        }
      ],
      "source": [
        "guess_human(model, sequences, idx_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lUAmxok08_lt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed Sequence: gaze coordinates implemented by the computer system so that scrolling function is performance based upon the screen gaze coordinates of the user's eye relative to a certain activation area on the screen. A method of controlling scrolling includes the acts of finding a screen gaze coordinates on the screen\n",
            "\n",
            "\n",
            "Option 1 < --- > gaze position of the initial state. The relationship between the operation of the output of the resulting time and the output and a predetermined number\n",
            "\n",
            "Option 2 < --- > and the quality of the breaths, and the health of the formation is greater than a modulator of a neural network and the wellhead or\n",
            "\n",
            "Option 3 < --- >, determining whether the screen gaze coordinate is within at least one activated control region, and activating scrolling to provide a display of information when\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***Correct***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ordering:  ['computer1', 'computer0', 'human']\n",
            "Diversity 0.51\n"
          ]
        }
      ],
      "source": [
        "guess_human(model, sequences, idx_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UQSbozpc8_oa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16192, 100)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings = get_embeddings(model)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SNsaAbYj8_rH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: network\n",
            "\n",
            "Word: network         Cosine Similarity: 1.0\n",
            "Word: channel         Cosine Similarity: 0.7754999995231628\n",
            "Word: networks        Cosine Similarity: 0.7745000123977661\n",
            "Word: system          Cosine Similarity: 0.7559999823570251\n",
            "Word: program         Cosine Similarity: 0.7541999816894531\n",
            "Word: cable           Cosine Similarity: 0.7419999837875366\n",
            "Word: now             Cosine Similarity: 0.7297999858856201\n",
            "Word: programming     Cosine Similarity: 0.7179999947547913\n",
            "Word: web             Cosine Similarity: 0.7138000130653381\n",
            "Word: line            Cosine Similarity: 0.6915000081062317\n"
          ]
        }
      ],
      "source": [
        "find_closest('network', embeddings, word_idx, idx_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "93hJKZ_YCDpm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: data\n",
            "\n",
            "Word: data            Cosine Similarity: 1.0\n",
            "Word: information     Cosine Similarity: 0.8185999989509583\n",
            "Word: numbers         Cosine Similarity: 0.683899998664856\n",
            "Word: database        Cosine Similarity: 0.6776000261306763\n",
            "Word: account         Cosine Similarity: 0.6575999855995178\n",
            "Word: report          Cosine Similarity: 0.6575999855995178\n",
            "Word: signals         Cosine Similarity: 0.6399999856948853\n",
            "Word: system          Cosine Similarity: 0.6377000212669373\n",
            "Word: statistics      Cosine Similarity: 0.6371999979019165\n",
            "Word: web             Cosine Similarity: 0.6359000205993652\n"
          ]
        }
      ],
      "source": [
        "find_closest('data', embeddings, word_idx, idx_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvSEGg-KCD0U",
        "outputId": "4781cb75-f7a8-4ddb-eb4b-6e346b5f1bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ],
      "source": [
        "import requests \n",
        "\n",
        "response = requests.get('https://google.com/') \n",
        "print(response) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjiL7ZB6CD9d",
        "outputId": "a013ade5-4d9d-4857-a841-2de629481311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYmOQvV3CEB9",
        "outputId": "57d0a721-413b-4f9f-c2c9-cb5cd8e74309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 37s 230ms/step - loss: 0.9236 - acc: 0.5120 - val_loss: 0.6880 - val_acc: 0.5408\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 34s 216ms/step - loss: 0.6722 - acc: 0.6507 - val_loss: 0.6687 - val_acc: 0.6464\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 34s 218ms/step - loss: 0.6326 - acc: 0.7555 - val_loss: 0.6250 - val_acc: 0.7286\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 34s 216ms/step - loss: 0.5547 - acc: 0.8034 - val_loss: 0.5238 - val_acc: 0.7898\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 34s 217ms/step - loss: 0.4366 - acc: 0.8421 - val_loss: 0.4298 - val_acc: 0.8306\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 35s 225ms/step - loss: 0.3537 - acc: 0.8716 - val_loss: 0.4254 - val_acc: 0.8460\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 34s 216ms/step - loss: 0.3106 - acc: 0.8899 - val_loss: 0.4046 - val_acc: 0.8602\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 37s 238ms/step - loss: 0.2723 - acc: 0.9032 - val_loss: 0.3965 - val_acc: 0.8664\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 34s 219ms/step - loss: 0.2468 - acc: 0.9155 - val_loss: 0.4144 - val_acc: 0.8678\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 34s 216ms/step - loss: 0.2246 - acc: 0.9227 - val_loss: 0.4471 - val_acc: 0.8720\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RNN-patent.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "fe3bab233825460445dbfeea6dcd52ef5815a3fa13c858fea07ac830aa383284"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
