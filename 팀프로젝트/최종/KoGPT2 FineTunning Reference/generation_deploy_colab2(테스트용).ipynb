{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "outputs": [],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGzJeg_x12T"
      },
      "source": [
        "# 구글 드라이브 연동하기\n",
        "모델 체크포인트 등을 저장해 둘 구글 드라이브를 연결합니다. 자신의 구글 계정에 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgSyL_BsVTfl"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC5OwyKMx_l9"
      },
      "source": [
        "# 각종 설정\n",
        "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fKybDwDqFIX5"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Either downstream_model_dir or downstream_model_checkpoint_fpath must be entered.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-13-33391e903c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mratsnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlpbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerationDeployArguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m args = GenerationDeployArguments(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpretrained_model_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"skt/kogpt2-base-v2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdownstream_model_dir\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ratsnlp\\nlpbook\\generation\\arguments.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, pretrained_model_name, downstream_model_dir, downstream_model_checkpoint_fpath)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownstream_model_checkpoint_fpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselected_fname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either downstream_model_dir or downstream_model_checkpoint_fpath must be entered.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"downstream_model_checkpoint_fpath: {self.downstream_model_checkpoint_fpath}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mException\u001b[0m: Either downstream_model_dir or downstream_model_checkpoint_fpath must be entered."
          ]
        }
      ],
      "source": [
        "from ratsnlp.nlpbook.generation import GenerationDeployArguments\n",
        "args = GenerationDeployArguments(\n",
        "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-generation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mThtbxyNyO"
      },
      "source": [
        "# 모델 로딩\n",
        "파인튜닝을 마친 GPT2 모델과 토크나이저를 읽어 들입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aFV031RZFRgD"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'args' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-c3710c30dba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m pretrained_model_config = GPT2Config.from_pretrained(\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "pretrained_model_config = GPT2Config.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        ")\n",
        "model = GPT2LMHeadModel(pretrained_model_config)\n",
        "fine_tuned_model_ckpt = torch.load(\n",
        "    args.downstream_model_checkpoint_fpath,\n",
        "    map_location=torch.device(\"cpu\"),\n",
        ")\n",
        "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3amlsjpFd9i"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    eos_token=\"</s>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVsdmThyV_p"
      },
      "source": [
        "# 인퍼런스 함수 선언\n",
        "인퍼런스 함수를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnzR9NMtFiAz"
      },
      "outputs": [],
      "source": [
        "def inference_fn(\n",
        "        prompt,\n",
        "        min_length=10,\n",
        "        max_length=20,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.0,\n",
        "        no_repeat_ngram_size=0,\n",
        "        temperature=1.0,\n",
        "):\n",
        "    try:\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids,\n",
        "                do_sample=True,\n",
        "                top_p=float(top_p),\n",
        "                top_k=int(top_k),\n",
        "                min_length=int(min_length),\n",
        "                max_length=int(max_length),\n",
        "                repetition_penalty=float(repetition_penalty),\n",
        "                no_repeat_ngram_size=int(no_repeat_ngram_size),\n",
        "                temperature=float(temperature),\n",
        "           )\n",
        "        generated_sentence = tokenizer.decode([el.item() for el in generated_ids[0]])\n",
        "    except:\n",
        "        generated_sentence = \"\"\"처리 중 오류가 발생했습니다. <br>\n",
        "            변수의 입력 범위를 확인하세요. <br><br> \n",
        "            min_length: 1 이상의 정수 <br>\n",
        "            max_length: 1 이상의 정수 <br>\n",
        "            top-p: 0 이상 1 이하의 실수 <br>\n",
        "            top-k: 1 이상의 정수 <br>\n",
        "            repetition_penalty: 1 이상의 실수 <br>\n",
        "            no_repeat_ngram_size: 1 이상의 정수 <br>\n",
        "            temperature: 0 이상의 실수\n",
        "            \"\"\"\n",
        "    return {\n",
        "        'result': generated_sentence,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP6ZAaSybge"
      },
      "source": [
        "# 웹서비스 개시\n",
        "아래처럼 실행해 인퍼런스 함수를 웹서비스로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_up1ARoHFwLN"
      },
      "outputs": [],
      "source": [
        "from ratsnlp.nlpbook.generation import get_web_service_app\n",
        "app = get_web_service_app(inference_fn)\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "generation-deploy-colab2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "fe3bab233825460445dbfeea6dcd52ef5815a3fa13c858fea07ac830aa383284"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
